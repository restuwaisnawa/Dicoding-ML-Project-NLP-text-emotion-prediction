# -*- coding: utf-8 -*-
"""Dicoding_Submission_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13OF9GDgCnvx-Rt4IdyxCay-y3LOuXDeQ

##**NLP ML SUBMISSION**

**I GEDE KADEK RESTU KARTANA WAISNAWA**

Dicoding ID: **restuwaisnawa**

Email: **restuwaisnawa@gmail.com**

Dataset link: https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp/
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#get dataset
df = pd.read_csv('train.txt', sep=";", names=["Description","Emotion"])
df.head()

#dataset check
df.info()

#total label data check
df['Emotion'].value_counts()

#one hot encoding
category = pd.get_dummies(df.Emotion)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='Emotion')
new_df.head()

#transform data in to array
desc = new_df['Description'].values
label = new_df[['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']].values

#train and validation set split
desc_train, desc_test, label_train, label_test = train_test_split(desc, label, test_size=0.2)

#data tokenizing
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(desc_train)

#data sequencing
train_sequence = tokenizer.texts_to_sequences(desc_train)
test_sequence = tokenizer.texts_to_sequences(desc_test)

#data padding
train_padded = pad_sequences(train_sequence)
test_padded = pad_sequences(test_sequence)

#making model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=16000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(6, activation='softmax'),
])

model.summary()

#model compile
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

#callbacks
class AutoAccuracyStop(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        current_accuracy = logs.get('accuracy')
        current_val_accuracy = logs.get('val_accuracy')

        if current_accuracy is not None and current_accuracy > 0.91 and \
           current_val_accuracy is not None and current_val_accuracy > 0.91:
            print('\nAccuracy has greater than 91%. Training Stopped')
            self.model.stop_training = True

auto_stop = AutoAccuracyStop()

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    verbose=1)

#model training
history = model.fit(
    train_padded, label_train, epochs=40,
    validation_data=(test_padded, label_test),
    verbose=2,
    callbacks=[auto_stop, early_stopping])

#plot accuracy
plt.plot(history.history['accuracy'], label='Training')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Plot Accuracy')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="lower right")
plt.show()

#plot loss
plt.plot(history.history['loss'], label='Training')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Plot Loss')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="upper right")
plt.show()